\chapter{Marco teórico}
\hrule \bigskip \vspace*{1cm}
%------------------------------------------------------------------------

\section{Desambiguación del sentido de las palabras (WSD)}
En general, la desambiguación del sentido de las palabras es el problema de seleccionar un sentido de un conjunto de posibilidades predefinidas para una palabra dada en un texto o discurso.
En los últimos años se han incrementado las investigaciones para crear métodos de WSD. A continuación, se describe la clasificación para métodos de WSD de acuerdo a los recursos que utilizan (figura \ref{fig:desambiguacion_WSD}).

  \begin{figure}[h!]
	  \begin{center}
    \includegraphics[angle=0, width=9cm]{Graficos/desambiguacion_WSD}
	  \caption{Clasificación de los métodos para WSD de acuerdo a los recursos que utilizan \cite{001}.}
    \label{fig:desambiguacion_WSD}
    \end{center}
\end{figure}

\section{Clasificación de sistemas en WSD}
\subsection{Métodos basados en conocimiento}
En esta categoría encontramos diferentes algoritmos para la etiquetación automática de sentidos. Normalmente, el rendimiento de estos métodos basados en conocimiento, es menor en comparación con los métodos basados en corpus. Pero con la salvedad de que los métodos basados en conocimiento tienen una amplia cobertura ya que pueden aplicarse a cualquier tipo de texto en comparación con los basados en corpus que sólo se pueden aplicar a aquellas palabras de las que se dispone de corpus anotados. A continuación, vamos a enumerar diferentes técnicas utilizadas por los métodos basados en conocimiento, aplicables sobre cualquier base de conocimiento léxica que defina sentidos de palabras y relaciones entre ellas. La base de conocimiento léxica más utilizada es WordNet (Miller (1995)). Vamos a describir 4 tipos diferentes de métodos basados en conocimiento:

\begin{itemize}
  \item Algoritmo de Lesk
  \item Similitud semántica
  \item Preferencias de selección
  \item Métodos Heurísticos
\end{itemize}

\subsubsection{Algoritmo de Lesk}
El algoritmo de Lesk \cite{002} es uno de los primeros algoritmos exitosos usados en la desambiguación de sentidos de palabras. Este algoritmo se basa en dos puntos principales: un algoritmo de optimización para WSD y una medida de similitud para las definiciones de los sentidos.
El primer punto es acerca de desambiguar palabras considerando la coherencia global del texto, esto es, encontrar la combinación de los sentidos que maximice la relación total entre los sentidos de todas las palabras.
Por ejemplo, para la oración \textit{My father deposits his money in a bank account} y considerando a lo más tres sentidos (véase la tabla \ref{tab:lesk01}), para cada palabra, la figura \ref{fig:algoritmo_lesk} muestra la representación gráfica del algoritmo original de Lesk.

  \begin{figure}[h!]
    \begin{center}
    \includegraphics[angle=0, width=9cm]{Graficos/algoritmo_lesk}
    \caption{Representación gráfica del algoritmo original de Lesk \cite{001}}
    \label{fig:algoritmo_lesk}
    \end{center}
  \end{figure}

  \begin{table}[t]
    \centering
      \begin{tabular}{|m{2cm}|m{12cm}|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      Palabra & Sentidos\\
      \hline
      \hline
      Father & 1: a male parent (also used as a term of address to your father); "his father was born in Atlanta".
      2: `Father' is a term of address for priests in some churches (especially the Roman Catholic Church or the Orthodox Catholic Church); “`Padre' is frequently used in the military”.
      3: a person who holds an important or distinguished position in some organization; "the tennis fathers ruled in her favor"; "the city fathers endorsed the proposal".\\
      \hline
      Deposit & 1: fix, force, or implant; "lodge a bullet in the table".
      2: put into a bank account; "She deposits her paycheck every month".
      3: put (something somewhere) firmly; "She posited her hand on his shoulder"; "deposit the suitcase on the bench"; "fix your eyes on this spot".\\
      \hline
      Money & 1: the official currency issued by a government or national bank; "he changed his money into francs".  \\
      \hline
      Bank & 1: a financial institution that accepts deposits and channels the money into lending activities; "he cashed a check at the bank"; "that bank holds the mortgage on my home".
      2: sloping land (especially the slope beside a body of water); "they pulled the canoe up on the bank"; "he sat on the bank of the river and watched the currents".
      3: a supply or stock held in reserve for future use (especially in emergencies) \\
      \hline
      Account& 1: a formal contractual relationship established to provide for regular banking or brokerage or business services; "he asked to see the executive who handled his account".
      2: the act of informing by verbal report; "he heard reports that they were causing trouble"; "by all accounts they were a happy couple".
      3: a record or narrative description of past events; "a history of France"; "he gave an inaccurate account of the plot to kill the president"; "the story of exposure to lead".\\
      \hline
    \end{tabular}
    \caption{Sentidos de las palabras (máximo tres) obtenidas de WordNet para la oración \textit{“My father deposits his money in a bank account”}.\cite{001}}
    \label{tab:lesk01}
  \end{table}

\clearpage
En el segundo punto, relacionado con la medida de similitud, Lesk sugiere usar el traslape entre las definiciones de los sentidos, es decir, contar el número de palabras que tienen en común.
Como ejemplo, para la oración, \textit{“My father deposits his money in the bank ac-count”} para medir la relación de las definiciones de los sentidos para la palabra \textit{“de-posit”} y \textit{“bank”} como Lesk lo propuso, es necesario contar las palabras en común en todas las definiciones. En este caso, comparando principalmente las tres definiciones de \textit{“deposit”} contra las tres definiciones de \textit{“bank”}. La relación entre los valores se muestra en la tabla \ref{tab:sentidos_palabras}.

  \begin{table}[]
    \centering
      \begin{tabular}{|m{4cm}|m{4cm}|m{4cm}|}
      \hline
      Sentido elegido para \textit{deposit} & Sentido elegido para \textit{bank} & Valor de relación (traslape de palabras)\\
      \hline
      1 & 1 & 0 \\
      \hline
      1 & 2 & 0 \\
      \hline
      1 & 3 & 0 \\
      \hline
      2 & 1 & 2 \\
      \hline
      2 & 2 & 1 \\
      \hline
      2 & 3 & 0 \\
      \hline
      3 & 1 & 1 \\
      \hline
      3 & 2 & 0 \\
      \hline
      3 & 3 & 0 \\
      \hline
     \end{tabular}
   \caption{Valores de relación para las definiciones de sentidos de las palabras \textit{“deposit”} y \textit{“bank”}.\cite{001}}
   \label{tab:sentidos_palabras}
  \end{table}

Este algoritmo tiene dos limitaciones, por un lado, la limitación principal de la medida de similitud propuesta por Lesk, es que las glosas del diccionario, regularmente, son muy cortas y no incluyen el vocabulario suficiente para identificar los sentidos relacionados \cite{003}. Por otro lado, mientras más palabras tenga el texto, y más sentidos por cada palabra, mayor será el número de combinaciones de sentidos, haciéndolo prácticamente prohibitivo para una búsqueda exhaustiva que garantice encontrar el óptimo global exacto. Por ejemplo, para una oración de 16 palabras de contenido, donde cada palabra contiene siete sentidos (números cercanos a los observados en el corpus de SemCor), existen 716 posibles combinaciones a escoger, de las cuales una será seleccionada.
Debido a estas dos limitaciones, diferentes modificaciones al algoritmo original han sido propuestas para mejorar los resultados en la desambiguación de sentidos de palabras, las cuales se describen en la siguiente sección.

\begin{itemize}
  \item \textbf{Lesk simple o Lesk simplificado} \\
    Para reducir el espacio de búsqueda del algoritmo original de Lesk, Kilgarriff y Rosenzweig \cite{004} propusieron una variación del algoritmo original de Lesk, conocido como algoritmo de \textbf{Lesk simplificado o Lesk simple}, donde los sentidos de las pala-bras en el texto son determinados uno a uno encontrando el mayor traslape entre los sentidos de las definiciones de cada palabra con el contexto actual, véase la figura \ref{fig:lesk_simple}. En lugar de buscar asignar, simultáneamente, el significado de todas las palabras en un texto dado, este enfoque determina el sentido de las palabras uno a uno, por lo que se evita la explosión combinatoria de sentidos.

    \begin{figure}[h!]
      \begin{center}
      \includegraphics[angle=0, width=10cm]{Graficos/lesk_simple}
      \caption{Representación gráfica del algoritmo de Lesk simplificado \cite{001}}
      \label{fig:lesk_simple}
      \end{center}
    \end{figure}

    \item \textbf{Templado simulado (Simulated Annealing)} \\
    El método de templado simulado es una técnica para la resolución de problemas de optimización combinatoria a gran escala. El nombre de este algoritmo es una analogía del proceso metalúrgico en el cuál, el metal se enfría y se templa. La característica de este fenómeno es que en el enfriamiento lento alcanza una composición uniforme y un estado de energía mínimo, sin embargo, cuando el proceso de enfriamiento es rápido, el metal alcanza un estado amorfo y con un estado alto de energía. En templado simulado la variable \textbf{T} corresponde a la temperatura que decrece lentamente hasta encontrar el estado mínimo.
    El proceso requiere una función \textbf{E}, la cual representa el estado de energía de cada configuración del sistema. Es esta función la que se intenta minimizar. A grandes rasgos el algoritmo funciona de la siguiente manera: se selecciona un punto inicial y además se escoge otra configuración de manera aleatoria, se calcula para ambas con-figuraciones su valor \textbf{E}, si el nuevo valor es menor que el seleccionado como punto inicial, entonces el inicial es remplazado por la nueva configuración. Una característica esencial del templado simulado es que, existe el caso en el que la nueva configuración es mayor a la configuración obtenida anteriormente, y la nueva es seleccionada. Esta decisión es tomada de manera probabilística y permite salir de algún mínimo local. Una vez que el método mantenga la misma configuración por un determinado tiempo, dicha configuración es escogida como la solución.
    Cowie et al. \cite{005}, basándose en el algoritmo de Lesk, utilizó este método para desambiguación de sentidos de palabras de la siguiente forma:
    \begin{enumerate}
      \item El algoritmo define una función \textbf{E} para la combinación de sentidos de palabras en un texto dado.
      \item Se calcula \textbf{E} para la configuración inicial \textbf{C}, donde \textbf{C} es el sentido más frecuente para cada palabra.
      \item Para cada iteración, se escoge aleatoriamente otra configuración conocida como \textbf{C’}, y se calcula su valor de \textbf{E}. Si el valor de \textbf{E} para \textbf{C’} es menor que el de \textbf{C} entonces se elige \textbf{C’} como configuración inicial.
      \item  La rutina termina cuando la configuración de sentidos no ha cambiado en un tiempo determinado.
    \end{enumerate}

  \item \textbf{Medida de Lesk Adaptada}\\
    Lesk propuso medir la similitud entre sentidos contando el traslape de palabras. La limitación principal de esta técnica es que las glosas del diccionario, por lo general, son muy breves, de tal manera que no incluyen suficiente vocabulario para identificar los sentidos relacionados. En \cite{006} se sugiere una adaptación del algoritmo basado en WordNet. Esta adaptación consiste en tomar en cuenta las glosas de los vecinos de la palabra a desambiguar, explotando los conceptos jerárquicos de WordNet, de tal ma-nera que las glosas de los vecinos son expandidas incluyendo a su vez las glosas de las palabras con las cuales se encuentran relacionadas mediante las diversas jerarquías que presenta WordNet. Así mismo, sugieren una variación en la manera de asignar el puntaje a una glosa, de tal manera que si “n” palabras consecutivas son iguales en ambas glosas, estas deberán de tener mayor puntaje que aquel caso en el que sólo coincide una sola palabra en ambas glosas.
    Supongamos que \textit{bark} (ladrido o corteza) es la palabra que se desea desambiguar y sus vecinos son \textit{dog} (perro) y \textit{tail} (cola). El algoritmo original de Lesk verifica las coincidencias en las glosas de los sentidos de \textit{dog} con las glosas de \textit{bark}. Luego verifica las coincidencias en las glosas de \textit{bark} y \textit{tail}. El sentido de \textit{bark} con el máximo número de coincidencias es seleccionado. La adaptación del algoritmo de Lesk considera estas mismas coincidencias y añade además las glosas de los sentidos de los conceptos que se encuentran relacionados semántica o léxicamente a \textit{dog}, \textit{bark} y \textit{tail}, de acuerdo a las jerarquías de WordNet.
\end{itemize}

\subsection{Métodos no supervisados basados en corpus}
El desarrollo de métodos que tratan de resolver el problema de la ambigüedad léxica ha supuesto la aparición de diferentes algoritmos que utilizan una serie de recursos diferentes. Podemos encontrar desde sistemas que utilizan técnicas de enriquecimiento de conocimiento utilizando diccionarios, tesauros o jerarquías de conceptos (los llamados basados en conocimiento), hasta sistemas que utilizan la información de textos anotados semánticamente (los llamados sistemas supervisados basados en corpus). El único inconveniente de estos sistemas es que es necesario la creación de textos, diccionarios u otras fuentes de información, de forma manual.
Esto supone un gran costo en su obtención y mantenimiento, además de generar dificultades cuando se tratan de anotar textos muy extensos, de un nuevo dominio o de un lenguaje diferente. Para evitar esta dependencia se han desarrollado dos aproximaciones diferentes. La primera de ellas trata de establecer distinciones entre sentidos basándose en su distribución, determinando por tanto que, palabras que aparecen en contextos similares deben tener sentidos similares \cite{007} y \cite{008}. La segunda aproximación está basada en equivalencias de traducción en corpus paralelos, los cuales identifican traducciones de una palabra en un lenguaje determinado cuya traducción depende del sentido de esa palabra en el lenguaje origen. Estas traducciones dependientes del sentido de una palabra pueden ser utilizadas como una recopilación de sentidos para esa palabra en el lenguaje origen.
Una de las claves de los métodos basados en distribución, es que no utilizan ningún recopilatorio de sentidos, únicamente clasifican palabras basándose en sus contextos observados en los corpus. Esta es una alternativa a los métodos que dependen de la anotación de corpus y que están restringidos a aquellas palabras que un experto ha clasificado para sus distintos sentidos. En todo caso, a pesar de que exista un repertorio de sentidos, su utilidad depende de las aplicaciones sobre las que se aplique. Las aproximaciones distribucionales no asignan sentidos a las palabras, pero sí permiten discriminar entre los sentidos de una palabra identificando clusters en contextos similares, donde cada cluster muestra que una palabra se está utilizando con un sentido determinado. Estos métodos presentan una aproximación diferente a la tarea tradicional de WSD, la cual clasifica palabras con respecto a un repertorio de sentidos existente. Los métodos basados en equivalencias de traducción se basan en el hecho de que los sentidos diferentes de una palabra en un lenguaje origen se pueden traducir en palabras diferentes en el lenguaje destino. Estas aproximaciones tienen dos propiedades. Primero, automáticamente derivan un repertorio de sentidos que hace distinciones relevantes para los problemas de traducción automática. Segundo, un corpus etiquetado basado en estas distinciones puede ser creado automáticamente y utilizado como corpus de entrenamiento para los métodos tradicionales de aprendizaje supervisado.
Una de las ventajas de utilizar métodos no supervisados basados en corpus, es que no se basan en ningún diccionario, repositorio de sentidos, tesauro, etc. De forma que no están restringidos a la interpretación de sentidos que el autor del diccionario haya impuesto. Ya que, es muy habitual que diferentes diccionarios aporten una distinción de sentidos más fina o más compacta, según la finalidad para la que estén creados. Al evitar hacer uso de estos recursos, se garantiza la adaptabilidad de estos sistemas a diferentes campos o ámbitos. Otra ventaja no menos importante, es que estos métodos son independientes del lenguaje. Es decir, son fácilmente adaptables a cualquier idioma que disponga de un corpus sobre el que obtener información.

\subsubsection{Métodos distribucionales}
Este tipo de métodos identifican las palabras que suelen aparecer en contextos similares, sin necesidad de utilizar un repositorio de sentidos. En \cite{009} por ejemplo, se realiza el proceso de desambiguación en dos pasos. El primer paso, es construir clusters que comparten características similares. El segundo paso, es etiquetar cada cluster con una definición que establezca el sentido de la palabra dentro de ese contexto. Esta es una visión completamente diferente del concepto general de WSD, donde los sentidos se suponen conocidos antes de comenzar el proceso de desambiguación. Esta nueva visión de “discriminación y etiquetación” corresponde a la forma ideal de obtener la definición de una palabra (lexicografía). Un lexicógrafo, seleccionaría diferentes contextos de una palabra determinada, a partir de un corpus extenso y representativo para el usuario final. Por ejemplo, si hablamos de un diccionario para niños, el corpus consistiría en textos escritos para niños. Y si hablamos de un diccionario sobre un dominio específico el corpus deberían ser textos de esa especialidad en particular. De esta forma el lexicógrafo, dividiría los contextos en los que aparece la palabra a estudiar en diferentes clusters, discriminando los diferentes sentidos que puede adoptar esa palabra, sin tener ninguna idea preconcebida de cuántos sentidos puede adoptar. El resultado de la discriminación es un número determinado de clusters que establecen los diferentes sentidos de la palabra, obtenidos estos a partir del corpus de entrada. A partir de aquí, se debe estudiar cada cluster y obtener una definición que actúe como una etiqueta o un sentido específico para la palabra. Esta última parte, la de asignar una definición concreta a la palabra en cada cluster es la más problemática, dado que en muchas ocasiones es difícil establecer una definición a partir de los contextos. Una posible solución sería identificar el conjunto de palabras que aparecen en un cluster y utilizarlas como una aproximación al sentido de la palabra. Por ejemplo, si tenemos la palabra “línea” y un cluster con: “teléfono”, “llamada”, “ocupada”, “móvil”. En este caso, estas palabras son indicativas del sentido asociado a este cluster. De esta forma, si los métodos no supervisados basados en corpus son desarrollados eficientemente, el resultado podría llegar a ser un proceso independiente del lenguaje que resuelve el problema de la ambigüedad sin tener que recurrir a un repositorio de sentidos. Existen dos aproximaciones distintas para los métodos distribucionales: Discriminación basada en tipos. Estos métodos identifican conjuntos (o clusters) de palabras que pueden estar relacionadas entre sí debido a su aparición en contextos similares. Normalmente se basan en medidas de similitud entre vectores de co-ocurrencia. Discriminación basada en tokens. Estos métodos agrupan todos los contextos donde una palabra determinada aparece, basándose en la similitud de estos contextos.

\subsubsection*{Discriminación basada en tipos}
En el caso de los métodos de discriminación basados en tipos, es necesario disponer de corpus extensos para poder extraer la similitud entre diferentes contextos donde aparece la palabra a desambiguar. En estos métodos la representación más utilizada se basa normalmente en la contabilización de co-ocurrencias o en medidas de asociación entre palabras. Usando esta información es posible identificar otras palabras que aparecen en contextos similares y por tanto pueden tener sentidos similares. De esta forma, se pueden extraer los distintos sentidos que puede adoptar una palabra polisémica. Por ejemplo, si seleccionamos la palabra “línea” que puede tener varios sentidos (línea telefónica, trazo, premio en el juego del bingo, etc), y ésta aparece en dos contextos distintos: contexto1 (dibujo, trazo, color, coordenada) y contexto2 (auricular, teléfono, comunicar, llamada). Podemos establecer a partir de las palabras extraídas del contexto, que en el primer caso “línea” hace referencia a un trazo en un dibujo, y en el segundo caso, hace referencia a una línea telefónica. Como ya se ha mencionado anteriormente, los métodos distribucionales basados en tipos necesitan de corpus bastante extensos. Es por ello, que la representación del espacio contextual se realizará en matrices de NxN dimensiones, donde N, es el número de palabras en el corpus. Cada celda de esta matriz contiene el número de veces que las palabras representadas en cada columna y fila co-ocurren dentro de una ventana de un tamaño especificado. Cuando no importa el orden en el que aparecen las palabras la frecuencia será la misma, pero si hablamos de bigramas, donde el orden sí importa, el valor de las celdas será distinto. Por tanto, si el orden no importa, se tendrá   cuadrada y simétrica. Sin embargo, si tenemos en cuenta el orden de aparición de las palabras, tendremos una matriz rectangular y no simétrica. Para estas matrices de co-ocurrencia, las celdas pueden almacenar el número de veces que dos palabras co-ocurren, o también pueden tomar valores más complejos. Por ejemplo, las celdas de una matriz de co-ocurrencia pueden contener el valor de diferentes medidas de asociación como: log-likelihood ratio \cite{010} o Información Mutua \cite{011}. Estas medidas indican el grado en que dos palabras co-ocurren con respecto a las demás palabras del corpus. En el caso de la medida del log-likelihood ratio partimos de una tabla 2 x 2 como sigue a continuación Tabla \ref{tab:log_likelihood}.

\begin{table}
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
     & Corpus 1 & Corpus 2 & Total \\
    \hline
    Frecuencia de la palabra & a & b & a + b \\
    \hline
    Frecuencia de otras palabras & c - a & d - b & c + d - a - b \\
    \hline
    Total & c & d & c + d \\
    \hline
  \end{tabular}
  \caption{Tabla 2x2 para log-likelihood ratio \cite{012}}
  \label{tab:log_likelihood}
\end{table}

En la Tabla \ref{tab:log_likelihood} se extraen las frecuencias relativas de una palabra entre dos corpus. Se denota por c al número de palabras total del corpus1 y por d al número de palabras total del corpus2 (N en total). Los valores de a y b son denotados como valores observados (O). Por último, queda por definir los valores esperados (E), según la Fórmula \ref{eq:dbt1}

\begin{equation}
  E_i=\frac{N_i\displaystyle\sum_{i}^{} O_i}{\displaystyle\sum_{i}^{} N_i}
  \label{eq:dbt1}
\end{equation}

Para la Tabla \ref{tab:log_likelihood} $N_1 = c$ y $N_2 = d$. Por lo tanto, para la palabra que estamos tratando:

\begin{equation}
  E_1=\frac{c*(a+b)}{(c+d)}\hspace{1cm} \text{y}\hspace{1cm} E_2=\frac{d*(a+b)}{(c+d)}
  \label{eq:dbt2}
\end{equation}

Los cálculos para obtener los valores esperados tienen en cuenta el tamaño de los dos corpus. Por tanto, no es necesario normalizar los valores, pudiendo aplicar a continuación la medida del log-likelihood según la Fórmula \ref{eq:dbt3}

\begin{equation}
  -2\ln\lambda=2\displaystyle\sum_{i}^{} O_i\ln \left(\frac{O_i}{E_i}\right)
  \label{eq:dbt3}
\end{equation}

La Fórmula \ref{eq:dbt3} es equivalente a calcular el log-likelihood ratio $G^2$ como sigue:

\begin{equation}
  G^2=2*\left(a*\ln \left(\frac{a}{E_1}\right)\right)+\left(b*\ln \frac{b}{E_2}\right)
  \label{eq:dbt4}
\end{equation}

Si los valores esperados y los observados son comparables, el valor de $G^2$ estará próximo a 0, lo que significa que la palabra ha aparecido junto a otra por casualidad, y no están relacionadas entre sí. Si se obtiene un valor mayor que 0, significa que los valores observados difieren en gran medida de los valores esperados, por lo que las palabras estarán fuertemente relacionadas entre sí.
Una vez decidido el tipo de medida a utilizar para establecer la co-ocurrencia entre distintas palabras y construida la matriz de co-ocurrencia, cada palabra será representada como un vector de N-dimensiones. A partir de cada vector obtenido, se puede medir la similitud contextual entre dos palabras obteniendo el coseno entre los vectores. Para el cálculo del coseno entre dos vectores se utiliza la Fórmula \ref{eq:dbt5}

\begin{equation}
  \cos (\vec{x},\vec{y})=\frac{\vec{x}\cdot\vec{y}}{|\vec{x}|\times |\vec{y}|}
  \label{eq:dbt5}
\end{equation}

Continuando con la definición de métodos distribucionales basados en tipos, encontramos distintos algoritmos que pueden ser aplicados. En esta sección vamos a tratar dos de estos algoritmos: Análisis de la Semántica Latente (LSA) \cite{013} y Clustering by Committee (CBC) \cite{014}. Mediante el algoritmo de LSA se representa un corpus en un espacio multidimensional, usando vectores. Cada vector representará el contexto en el cual aparece una palabra. En el caso de LSA no se hacen distinciones entre los distintos sentidos de una palabra polisémica, es decir, se formará un único vector para cada palabra, aunque ésta tenga varios sentidos diferentes. Usando la información del contexto, se podrá determinar, por ejemplo, que palabras como: coche, automóvil, auto... están relacionadas semánticamente.

Cuando hablamos de LSA, debemos pensar en la representación del conocimiento como matrices de [palabras-contextos]. Para medir el grado de similitud de una palabra con respecto a otras palabras del contexto, se utiliza la medida del coseno entre vectores. Además de poder comparar palabras y contextos, también se puede medir el grado de similitud entre oración-oración, contexto-contexto... simplemente calculando el vector resultado de la unión de cada uno de los vectores que conforman las palabras de la oración, del contexto, etc. Mediante el algoritmo de CBC se pueden detectar clusters de palabras relacionadas con los distintos sentidos de una palabra polisémica. Por ejemplo, para la palabra “muñeca” el algoritmo de CBC podría identificar dos clusters, uno asociado con el sentido de juguete, con palabras como juego, entretenimiento, niños, cochecito, etc, y otro cluster asociado con el sentido de parte del cuerpo humano, con palabras como brazo, extremidad, articulación, etc. Por lo tanto, con el algoritmo de CBC se pueden detectar palabras sinónimas asociadas a los diferentes sentidos de una palabra. Ambos algoritmos, tanto LSA como CBC, utilizan representaciones multidimensionales de co-ocurrencia de palabras.

\subsubsection*{Discriminación basada en tokens}

El objetivo de este tipo de métodos es agrupar los contextos en los que una palabra aparece bajo el mismo sentido. A continuación, se van a describir métodos que utilizan características de primer y segundo orden. Las características de primer orden ocurren directamente en un contexto que está siendo clasificado, mientras que las características de segundo orden son aquellas que ocurren junto con una de primer orden, pero no ocurren en el contexto que está siendo clasificado. En primer lugar, es necesario establecer cómo se van a representar los contextos que van a ser clasificados en clusters. Al igual que para los sistemas supervisados, los contextos contienen la palabra a desambiguar con la excepción de que esta no tiene asignado ningún sentido. La premisa de los métodos basados en tokens es que si una palabra aparece en contextos similares ésta ha de tener el mismo sentido.
Uno de los primeros métodos que utilizó discriminación basada en tokens fue una adaptación del algoritmo de LSA usando características de segundo orden \cite{009}. En este caso, la representación de la matriz de co-ocurrencia en lugar de utilizar palabras utiliza contextos completos usando co-ocurrencias de segundo orden de características léxicas. Una palabra tiene una co-ocurrencia de segundo orden con otra, cuando ambas no aparecen juntas, pero ambas sí aparecen junto a otra palabra frecuentemente. Por ejemplo, en “policía de tráfico” y “accidente de tráfico”, la palabra “policía” es una co-ocurrencia de segundo orden de “accidente”, porque ambas co-ocurren en primer orden con “tráfico”. Otro método que utiliza esta aproximación es el de \cite{015}. En este caso, utilizan un conjunto reducido de características de primer orden para crear matrices que muestran la similitud entre contextos. Estas características se extraen a partir de las palabras que se encuentran alrededor de la palabra a desambiguar e incluyen etiquetas sintácticas y palabras co-ocurrentes.
El problema de este tipo de métodos es la forma de evaluación de los resultados. Debido a que la discriminación no parte de un conjunto preestablecido de sentidos, no se puede evaluar la forma en que los nuevos sentidos son descubiertos, sobretodo si se está trabajando en un dominio específico.

\subsection{Métodos supervisados basados en corpus}

Los métodos supervisados realizan la desambiguación de forma automática a partir de modelos o reglas obtenidas a partir de textos anotados previamente. Cuando hablamos de textos anotados, nos referimos a textos cuyo contenido ha sido etiquetado de forma manual. En este caso, la etiquetación se corresponde tanto a la parte de semántica como a la parte sintáctica. Ya que, como se comentó anteriormente se debe identificar el tipo de categoría sintáctica de una palabra, para poder establecer su sentido semántico. En líneas generales los pasos a seguir por un método supervisado son los siguientes:

  \begin{enumerate}
    \item Seleccionar un conjunto de ejemplos que muestren las distintas clasificaciones de cada elemento.
    \item Identificar patrones asociados a cada elemento.
    \item Generalizar los patrones en reglas.
    \item Aplicar las reglas para clasificar nuevos elementos.
  \end{enumerate}

Dentro de este tipo de métodos cabe destacar las técnicas basadas en Aprendizaje Automático \cite{016}. Estas técnicas han sido ampliamente utilizadas en tareas de PLN obteniendo un éxito considerable. Los problemas iniciales de PLN sobre los que fueron aplicados este tipo de métodos estadísticos y de aprendizaje automático, fueron aquellos vinculados a la resolución de la ambigüedad léxica. En este tipo de tareas, se debe seleccionar de entre un conjunto de alternativas, la interpretación correcta para una palabra en un contexto determinado. Podemos destacar tareas tales como: selección de palabras en reconocimiento de voz, traducción automática, desambiguación automática, resolución de co-referencias, etc. Este tipo de tareas se consideran adecuadas para un sistema de aprendizaje automático porque pueden ser vistas como problemas de clasificación, donde el sistema de aprendizaje trata de etiquetar (clasificar) una serie de elementos, utilizando una de entre varias categorías (clases). En este caso, la base de conocimiento del sistema está formada por ejemplos previamente etiquetados. Actualmente, las técnicas de aprendizaje automático han sido aplicadas a otros problemas de PLN, los cuales, no se reducen a un simple problema de clasificación. Dentro de estas nuevas aplicaciones encontramos: etiquetación de secuencias (con entidades, categorías sintácticas, etc) y asignación de estructuras jerárquicas (árboles sintácticos, conceptos complejos en extracción de información, etc). En estos casos, se parte de un problema complejo que puede ser descompuesto en esquemas de decisión simples o se pueden generalizar los conjuntos de clasificación para trabajar directamente con representaciones y salidas complejas. En relación a WSD, en los últimos diez años, la técnica de aprendizaje supervisado, a partir de ejemplos, ha sido una de las que mejores resultados ha obtenido. En este caso, los modelos estadísticos o de clasificación se obtienen a partir de corpus anotados semánticamente. Normalmente, los métodos supervisados han obtenido mejores resultados que los no supervisados. Esta afirmación queda demostrada a la vista de los resultados conseguidos en las últimas competiciones realizadas para la evaluación de métodos de análisis semántico \cite{017}, \cite{018}. Sin embargo, estos métodos tienen un grave problema, la necesidad de disponer de corpus lo bastante extensos para poder entrenar los sistemas. A menudo, escasean los corpus anotados debido a su costoso proceso de anotación manual, es el conocido problema del cuello de botella de la adquisición de conocimiento. Esta restricción afecta en gran medida a los sistemas, ya que no tienen la materia prima necesaria para poder trabajar.

\subsubsection*{El proceso de clasificación en aprendizaje supervisado}

El objetivo principal en el aprendizaje supervisado para la tarea de clasificación consiste en inducir a partir de un conjunto de entrenamiento $C$, una aproximación (o hipótesis) $h$ de una función no conocida $f$ que mapea a partir de un espacio de entrada \textit{E} a un espacio de salida $S = 1; …;K$

El conjunto de entrenamiento contiene \textit{m} ejemplos de entrenamiento, $C=(\vec{e}^1,y^1),...,(\vec{e}^m,y^m)$, pares $(\vec{e},y)$. Donde $\vec{e}$ pertenece a \textit{E} y $y=f(\vec{e})$. El componente $\vec{e}$ de cada ejemplo es normalmente un vector $\vec{e}=(e_1,...,e_n)$ cuyos componentes, llamados atributos (features) describen información relevante acerca del ejemplo. Los valores del espacio de salida S asociados con cada ejemplo de entrenamiento se llaman clases (categorías). Por lo tanto, cada ejemplo de entrenamiento está completamente descrito por un conjunto de pares atributo-valor y una etiqueta de clase.\\
Según la teoría del aprendizaje estadístico \cite{019}), la función $f$ se considera como una función de distribución de probabilidad $P(X;Y)$ y los ejemplos de entrenamiento se consideran como una muestra de esa distribución. Además, $X$ se identifica normalmente con $R^n$, y cada ejemplo $\vec{x}$ como un punto en $R^n$ con un valor real en cada dimensión. Estas son las dos posibles notaciones que podemos encontrar en este tipo de sistemas.\\
Dado un conjunto de entrenamiento $C$, un algoritmo de aprendizaje induce un clasificador denotado como $h$, el cual es utilizado como una hipótesis sobre la verdadera función $f$. A partir de aquí el algoritmo de aprendizaje puede seleccionar entre un conjunto de posibles funciones $H$, a las que se llama \textit{espacio de hipótesis}. Los algoritmos de aprendizaje se diferencian en base a dos rasgos: el tipo de espacio de hipótesis que manejan: funciones lineales, funciones radiales, etc. O el tipo de algoritmo de selección que utilizan para decidir cuál de las hipótesis es la mejor con respecto al corpus de entrenamiento: simplicidad, margen máximo, etc.\\
Dados nuevos vectores $\vec{x},h$ se utiliza para predecir los correspondientes valores $y$.En este caso, se clasifican los nuevos ejemplos, y el resultado se prevee que coincida con $f$ en la mayoria de los casos, o de forma equivalente, que conlleve al menor número de errores. La forma de estimar el grado de error en aquellos ejemplos nunca vistos anteriormente se denomina \textit{error de generalización}. Este tipo de errores no pueden ser minimizados por el algoritmo de aprendizaje, dado que la función $f$ o la distribución $P(X;Y)$ son desconocidas. . Por lo tanto, es necesario un principio de inducción. La forma más común de proceder es minimizar el denominado \textit{error de entrenamiento}, es decir, el número de errores que encontramos en el conjunto de entrenamiento. Esta acción se conoce como la \textit{minimización del riesgo empírico} y proporciona una buena estimación del error de generalización con los suficientes ejemplos de entrenamiento. Sin embargo, para dominios con pocos ejemplos de entrenamiento, podemos ajustar demasiado los datos de entrenamiento y generalizar erróneamente. El riesgo de ajuste se ve incrementado cuando tenemos datos atípicos y ruido.

\subsubsection*{Clasificación de métodos de aprendizaje supervisado}

A continuación, se van a describir algunos de los distintos métodos de aprendizaje supervisado utilizados en WSD. Estos métodos son clasificados atendiendo a la forma que tienen de adquirir los modelos de clasificación.

\begin{itemize}
  \item \textbf{Métodos probabilísticos} \\
  Los métodos estadísticos normalmente estiman un conjunto de parámetros que determinan la probabilidad condicional de las categorías y los contextos (descritos mediante características). Estos parámetros se utilizan para asignar a cada nuevo ejemplo una categoría que maximice la probabilidad condicional a partir de las características observadas anteriormente. El clasificador más simple que existe es el denominado Naive Bayes Classifier (NBC) \cite{020}. En este modelo, hay un nodo que representa la variable de clase $C$ y un nodo para cada atributo $x_i$ del ejemplo (ver Figura 2.4)

  \begin{figure}[h!]
    \begin{center}
    \includegraphics[angle=0, width=10cm]{Graficos/bayesiano}
    \caption{Modelo clasificador bayesiano (naive) \cite{001}}
    \label{fig:bayesiano}
    \end{center}
  \end{figure}
  Se parte de la hipótesis de que los valores de los atributos se generan independientemente a partir de la clase $C$ de acuerdo con las distribuciones individuales $P(x_i|C)$. Para predecir la clase de un ejemplo, se elige la que maximiza la probabilidad de haber generado el ejemplo observado. Para ello, se utiliza una fórmula derivada a partir del teorema de Bayes. Este algoritmo ha sido usado en distintas tareas de PLN para resolver diversos problemas (categorización de documentos \cite{021}, corrección ortográfica \cite{022}, resolución de la ambiguedad semántica \cite{023}, \cite{024} ...) y a pesar de su extrema simplicidad, ha obtenido resultados notables. Además, utilizando el NBC se puede combinar información estadística de distintas fuentes, siempre que sean independientes. La fórmula general para obtener la clasificación según el clasificador bayesiano es la siguiente:

  \begin{equation}
    P(C|X_1,X_2,X_3,...,X_n)=\frac{P(X_1,X_2,X_3,...,X_n|C)\times P(C)}{P(X_1,X_2,X_3,...,X_n)}
    \label{eq:dbt6}
  \end{equation}

  Dado que los atributos son independientes con respecto a la clase $C$ se cumple que:

  \begin{equation}
    P(C|X_1,X_2,X_3,...,X_n|C)=\prod_{i}^{}P(X_i|C)
    \label{eq:dbt7}
  \end{equation}

  De esta forma, el clasificador bayesiano naive obtiene el siguiente resultado:

  \begin{equation}
    valor=arg max P(X_1|C)\times ... \times P(X_n|C) \times P(C)
    \label{eq:dbt8}
  \end{equation}

  Para el caso de WSD el sentido correcto para una palabra cualquiera C sería aquel que hiciera máximo el resultado de la Ecuación \ref{eq:dbt8}

  Por ejemplo, supongamos que tenemos 2000 instancias de la palabra \textit{“bank”}: 1500 para $bank1$ (financial) y 500 para $bank2$ (river). En este caso, las probabilidades para cada sentido serían:

  \begin{center}
  $P(S=1)=1500/2000=0,75$\\
  $P(S=2)=500/2000=0,25$
  \end{center}

  Dada la palabra \textit{“credit”} esta aparece 200 veces con $bank1$ y 4 veces con $bank2$.

  \begin{center}
    $P(X_1=credit)=204/2000=0,102$\\
    $P(X_1=credit|C=1)=200/1500=0,133$\\
    $P(X_1=credit|C=2)=4/500=0,08$
  \end{center}

  Dado un texto que contiene la palabra \textit{“credit”}:

  \begin{center}
    $P(C=1|X_1=credit)=(0,133 \times 0,75)/0,102=0,978$\\
    $P(C=2|X_1=credit)=(0,08 \times 0,25)/0,102=0,20$
  \end{center}

  Por tanto, se deduciría que el sentido correcto para \textit{“bank”} es el número 1. La efectividad del clasificador bayesiano \textit{“naive”} ha sido probada en diferentes estudios \cite{025}, \cite{026} que demuestran que este clasificador es tan bueno como cualquier otro método.

  \item \textbf{Métodos basados en reglas de discriminación} \\
  Este tipo de métodos utilizan las llamadas listas de decisión \cite{027} o árboles de decisión \cite{028}, \cite{029} donde se utilizan reglas asociadas a cada uno de los diferentes sentidos de una palabra. En este caso, dado un ejemplo a clasificar, el sistema selecciona una o más reglas que son satisfechas por las características del ejemplo y asigna un sentido basándose en sus predicciones.\\
  Concretamente, una lista de decisión es un conjunto ordenado de reglas de la forma (condición, clase, peso).\\
  En \cite{030} se utilizan listas de decisión para resolver un tipo específico de ambigüedad: los acentos en español y francés. En un trabajo posterior se aplicaron listas de decisión para WSD \cite{031}. En este estudio, cada condición de la lista se correspondía con una característica (\textit{“feature”}), donde los valores eran los sentidos de las palabras y los pesos se calculaban de acuerdo a una fórmula que estimaba la probabilidad de un sentido con respecto a una determinada característica. En \cite{032}, \cite{033} las listas de decisión se emplean en un sistema de WSD junto con una serie de nuevas características sintácticas (relaciones gramaticales instanciadas, relaciones gramaticales...) y semánticas (modelos de preferencias de selección). Este sistema se ha empleado para desambiguar textos en Euskera e Inglés. En el caso de los árboles de decisión las reglas de clasificación se generan en forma de una estructura n-aria de ramas de un árbol. \\
  Cada rama de un árbol de decisión representa una regla que comprueba un conjunto de características (nodos internos) y hace una predicción de la clase del nodo terminal. Este tipo de estructuras no se han empleado frecuentemente en WSD. En \cite{025} se utilizó el algoritmo de \cite{029} y se realizó un estudio comparativo con varios algoritmos de aprendizaje automático para WSD. Este estudio concluyó que los árboles de decisión no estaban entre los algoritmos que mejor resolvían el problema de WSD. A pesar de ello, en Senseval-1 \cite{034} presentó un sistema modificado de listas de decisión con algunas ramas condicionales que obtuvo muy buenos resultados en la tarea \textit{“English Lexical Sample”}.

  \item \textbf{Bootstrapping} \\
  Como se ha comentado anteriormente, el problema de los métodos basados en aprendizaje automático es la escasez de corpus anotados semánticamente. Para evitar este problema existe un método que requiere de un mínimo conjunto de elementos anotados (sistemas mínimamente supervisados), es el denominado método “de semilla” o bootstrapping \cite{035}, \cite{036}. La idea de este método es que a partir de un mínimo conjunto de ejemplos anotados se pueden realizar sucesivos aprendizajes que se alimentan incrementalmente con el conocimiento adquirido en el anterior. El término “semilla” proviene del inicio de tal proceso iterativo, que no necesita más que una mínima cantidad de conocimiento previo para comenzar el aprendizaje. Existen diferentes aproximaciones del método de bootstrapping: co-training y self-training. La idea básica para ambas aproximaciones es la siguiente:\\
  Se parte de un conjunto \textit{EE} de Ejemplos de Entrenamiento etiquetados y de un conjunto \textit{EN} de Ejemplos No Etiquetados. Se dispone de $C_i$ Clasificadores.

  \textbf{Paso 1.} Crear un conjunto de ejemplos $NE_0$, eligiendo $P$ ejemplos aleatorios de $NE$.\\
  \textbf{Paso 2.} Bucle de $I$ iteraciones:
  \begin{itemize}
    \item Entrenar los clasificadores $C_i$ sobre el conjunto etiquetado $EE$ y etiquetar el conjunto no etiquetado $NE_0$.
    \item Seleccionar los $M$ mejores ejemplos y añadirlos al conjunto $EE$, manteniendo la distribución de $EE$.
    \item Rellenar el conjunto $NE_0$ con ejemplos de $NE$, manteniendo $NE_0$ en un tamaño constante $P$.
  \end{itemize}

  Un ejemplo del método co-training lo encontramos en \cite{037} donde se utilizan dos clasificadores. Y un ejemplo del método self-training lo encontramos en \cite{038} donde se utiliza un único clasificador.

  \item \textbf{Métodos basados en redes neuronales.} \\
  Otro tipo de métodos utilizados para WSD son aquellos basados en redes neuronales, algoritmos genéticos, etc \cite{039}, \cite{040}. Una Red Neuronal Artificial (RNA) es un modelo de procesamiento de información que está inspirado en un sistema nervioso biológico \cite{041}.\\
  Este se compone de un gran número de elementos de procesamiento interconectados (neuronas) trabajando conjuntamente para resolver problemas específicos. Las RNA aprenden con ejemplos, es por tanto necesaria la utilización de un proceso de aprendizaje para su configuración. La principal ventaja de las RNA radica en la resolución de problemas demasiado complejos para tecnologías convencionales, problemas que no tienen un algoritmo de solución específico o que es muy difícil de encontrar. Entre estos problemas se encuentran el reconocimiento de patrones y pronósticos, clasificación de datos, optimización... Sin embargo, en el ámbito del procesamiento del lenguaje natural aún no han sido suficientemente explotadas \cite{042}. En \cite{043} se propone un sistema de WSD basado en el modelo de red neural de Kohonen \cite{044}, en su variante de aprendizaje por Cuantificación Vectorial (Learning Vector Quantification o LVQ). Como recursos lingüísticos para el aprendizaje de la red se utiliza el corpus de Semcor, que está etiquetado con los sentidos de WordNet y el conjunto de párrafos artificiales generado a partir de todas las relaciones de WordNet. Además, integra el modelo de espacio vectorial (con vectores obtenidos a partir de Semcor) con LVQ para definir las categorías de la red. Este sistema participó en la tarea English Lexical Sample de Senseval-2 obteniendo una precisión del 59\%.

\end{itemize}

\subsection{Métodos híbridos}
Los métodos que se encuentran dentro de este grupo son aquellos que no pueden englobarse exactamente dentro de los grupos anteriores. Es decir, son aquellos que utilizan en el proceso de desambiguación tanto fuentes de conocimiento externas como corpus anotados o no anotados.\\
Un método que combina la utilización de diccionarios con corpus no anotados es el ideado por \cite{045}. Este método utiliza las definiciones de LDOCE para extraer las palabras que identifican cada sentido, construyendo así para cada sentido una lista de palabras representativas. Utilizando esta información y las oraciones del Brown Corpus \cite{046} no anotado, se expande el conjunto de palabras representativas obtenido a partir de LDOCE, de la siguiente forma: se extraen pares de palabras de cada oración del Brown Corpus y se determinan los conceptos co-ocurrentes mediante un algoritmo que obtiene una tabla de datos conceptuales co-ocurrentes. Esto permite producir un sistema que utiliza la información de recursos léxicos como un medio para reducir la gran cantidad de texto necesaria de los corpus de entrenamiento.\\
El proceso de desambiguación,  de una palabra polisémica $W$ sobre un contexto $C$ (la oración que contiene a la palabra), comienza dando valores a cada sentido $S$ de $W$, según la fórmula \ref{eq:dbt9}

\begin{equation}
  score(S,C)=score(CS,C')-score(CS,GlobalCS)
  \label{eq:dbt9}
\end{equation}

Donde $CS$ es el conjunto de palabras representativas de LDOCE pertenecientes al sentido $S,$ $C_0$ es el conjunto ampliado de palabras representativas y GlobalCS contiene las definiciones de cada concepto. A partir de estos valores y utilizando la Información Mutua entre los diferentes conjuntos de palabras representativas, se selecciona el sentido con mayor valor de Información Mutua. Este sistema obtuvo un 77\% de precisión sobre las 12 palabras utilizadas en el trabajo de \cite{047}.\\
Otros métodos destacables de este tipo son los publicados en: \cite{048}, \cite{049} y \cite{050}. Existen también otros métodos que utilizan la combinación de tesauros y corpus no anotados, como es el caso del método ideado en \cite{047}. Este método emplea la técnica de bootstrapping utilizando las palabras de las categorías del Roget’s Thesaurus, considerándolas etiquetadas semánticamente y esta información se va aumentando utilizando un corpus no anotado. Además, podemos encontrar también métodos que combinan diferentes fuentes léxicas estructuradas con corpus: \cite{051}, \cite{052}. Métodos que combinan WordNet y corpus: \cite{053}, \cite{054}, etc. En definitiva, el número de sistemas de WSD surgidos a partir de la combinación de diferentes fuentes de conocimiento es muy amplio y seríaa imposible citarlos exhaustivamente.

\section{Bolsa de Palabras}
\paragraph{}
BoVW es actualmente un método popular para el reconocimiento de objetos y escenas en visión por computadoras. A una imagen se le extraen los rasgos locales y pasa a ser considerada como una bolsa de rasgos (bag of features), es decir, ignorando las relaciones espaciales entre ellos. Como desventaja podemos mencionar que este no cuenta con un mecanismo eficiente y efectivo de codificación de la información espacial que existe para los rasgos. Un método basado en el BoVW clásico consiste en las siguientes etapas:

\begin{figure}[h!]
	\begin{center}
	\includegraphics[angle=0,width=9.5cm]{Graficos/diagrama_bloques}
	\caption{Diagramas de bloques de la \textit{Bolsa de Palabras}}
	\label{fig:diagrama_bloques}
  \end{center}
\end{figure}

\begin{enumerate}
	\item \textbf{Extracción de rasgos:} Los rasgos locales y sus descriptores correspondientes se extraen de parches locales de la imagen. Los dos descriptores visuales más usados son SIFT (LOWE, 2004) y SURF (VEDALDI and FULKERSON, 2010). Algunos métodos los extraen en ciertos puntos de interés detectados y otros obtienen los rasgos locales densamente, en posiciones regulares de la imagen por ejemplo PHOW (VEDALDI  and  FULKERSON,  2010).
	\item \textbf{Generar un diccionario y mapear los rasgos a palabras visuales:} Un diccionario visual es un método que divide el espacio de descriptores visuales en varias regiones. Los rasgos de una región corresponden a la misma palabra visual. Entonces, una imagen se codifica como un histograma de la frecuencia de ocurrencia de cada palabra visual. Esto se hace asignando a cada vector de rasgos de la imagen su región más cercana, de manera que al terminar el proceso se tenga la cantidad de vectores asignados a cada región y se asigna esa cantidad a la componente correspondiente a esa palabra visual en el histograma.
	\item \textbf{Entrenar y probar:} Varios métodos de aprendizaje por computadora pueden aplicarse para la representación de imágenes usada. SVM es frecuentemente usado como clasificador en modelos BoVW para el reconocimiento de objetos y escenas. Este fue el clasificador escogido para resolver el problema planteado, en conjunto con el kernel aditivo de intersección de histogramas debido a su utilidad y buen desempeño para representaciones basadas en histogramas.
\end{enumerate}

\section{Reductor de Dimensiones MDS}
\label{mds}
\paragraph{}

\textit{Multidimensional scaling} es un método para representar una matriz de disimilaridades en un cierto número de dimensiones. Se puede usar como una técnica de reducción de la dimensionalidad si se calculan las disimilaridades con respecto a la dimensión original y luego se representan en un espacio de dimensión menor.

La idea de MDS es que las distancias euclídeas en el espacio de llegada sean lo más cercanas posibles a las disimilaridades en el espacio original. Si tenemos observaciones $x_1, x_2..., x_N$ y una función de disimilaridad \textit{d}, la función de coste a optimizar en MDS podría ser:

\[C=\sum_{i \neq j}^{}(d(x_i,x_j)-||z_i-z_j||)^2\]

donde $z_i$ y $z_j$ son los representantes en el espacio de llegada de $x_i$ y $x_j$ respectivamente.
Esta función puede optimizarse mediante descenso del gradiente

Sin embargo, si nuestras disimilaridades son distancias euclídeas existe una solución analítica. Si nuestras observaciones originales se representan por la matriz X sabemos que la matriz de Gram se define como $B = XX^T$. Podemos obtener la matriz de Gram mediante nuestra matriz de distancias al cuadrado D.

\[B=\frac{-1}{2} * C_n * D * C_n\]

Donde $C_n = I_n - \frac{1}{n} 11^T$ sirve para centrar la matriz de distancias. Por tanto si descomponemos la matriz de Gram podremos recuperar la matriz X y con ella las coordenadas en el espacio de llegada. En este caso (usando distancias euclídeas) el resultado será equivalente al obtenido mediante PCA.

\section{KNN}
K-Nearest Neighbors es uno de los algoritmos de clasificación más básicos pero esenciales en Machine Learning. Pertenece al dominio de aprendizaje supervisado y encuentra una intensa aplicación en el reconocimiento de patrones, minería de datos y detección de intrusos.
Es ampliamente disponible en escenarios de la vida real ya que no es paramétrico, lo que significa que no hace suposiciones subyacentes sobre la distribución de datos (a diferencia de otros algoritmos como GMM, que asume una distribución gaussiana de los datos dados) .
Nos dan unos datos previos (también llamados datos de entrenamiento), que clasifican las coordenadas en grupos identificados por un atributo.
Como ejemplo, considere la figura \ref{fig:knn_01} que muestra una tabla de puntos de datos que contienen dos características:

\clearpage
\begin{figure}[h!]
	\begin{center}
	\includegraphics[angle=0,width=9cm]{Graficos/knn_01}
	\caption{Datos de entrenamiento en KNN}
	\label{fig:knn_01}
  \end{center}
\end{figure}

Ahora, dado otro conjunto de puntos de datos (también llamados datos de prueba), asigne estos puntos a un grupo analizando el conjunto de entrenamiento. Tenga en cuenta que los puntos no clasificados están marcados como 'Blanco', de acuerdo la la figura \ref{fig:knn_02}

\begin{figure}[h!]
	\begin{center}
	\includegraphics[angle=0,width=9cm]{Graficos/knn_02.png}
	\caption{Datos de prueba en KNN}
	\label{fig:knn_02}
  \end{center}
\end{figure}

\paragraph{Intuición}
Si trazamos estos puntos en un gráfico, podemos ubicar algunos grupos o grupos. Ahora, dado un punto sin clasificar, podemos asignarlo a un grupo observando a qué grupo pertenecen sus vecinos más cercanos. Esto significa que un punto cercano a un grupo de puntos clasificados como 'Rojo' tiene una mayor probabilidad de ser clasificado como "Rojo".
Intuitivamente, podemos ver que el primer punto (2.5, 7) debe clasificarse como 'Verde' y el segundo punto (5.5, 4.5) debe clasificarse como 'Rojo'.

\paragraph{Algoritmo}
Sea m el número de muestras de datos de entrenamiento. Sea p un punto desconocido.
\begin{enumerate}
	\item Almacene las muestras de entrenamiento en una matriz de puntos de datos arr[ ]. Esto significa que cada elemento de esta matriz representa una tupla (x, y).
	\item Haz el conjunto S de las K distancias más pequeñas obtenidas. Cada una de estas distancias corresponde a un punto de datos ya clasificado.
	\item Devuelve la etiqueta mayoritaria entre S.
\end{enumerate}

\paragraph{}
K se puede mantener como un número impar para que podamos calcular una clara mayoría en el caso de que solo sean posibles dos grupos (por ejemplo, rojo/azul). Con el aumento de K, obtenemos límites más suaves y definidos a través de diferentes clasificaciones. Además, la precisión del clasificador anterior aumenta a medida que aumentamos la cantidad de puntos de datos en el conjunto de entrenamiento.
